.\" Man page for bristlenose
.\" Generated from cli.py help text
.TH BRISTLENOSE 1 "February 2026" "bristlenose 0.9.3" "User Commands"
.SH NAME
bristlenose \- user-research transcription and quote extraction engine
.SH SYNOPSIS
.B bristlenose
.RI [ command ]
.RI [ options ]
.br
.B bristlenose run
.I input-dir
.RB [ \-o
.IR output-dir ]
.RB [ \-p
.IR project-name ]
.RI [ options ]
.br
.B bristlenose transcribe
.I input-dir
.RB [ \-o
.IR output-dir ]
.RI [ options ]
.br
.B bristlenose analyze
.I transcripts-dir
.RB [ \-o
.IR output-dir ]
.RI [ options ]
.br
.B bristlenose render
.RI [ output-dir ]
.RI [ options ]
.br
.B bristlenose doctor
.br
.B bristlenose configure
.I provider
.RB [ \-k
.IR key ]
.br
.B bristlenose help
.RI [ topic ]
.SH DESCRIPTION
.B bristlenose
processes user-research interview recordings into structured, themed, timestamped
quotes. It transcribes audio locally using Whisper, extracts and enriches quotes
via LLM (Claude, ChatGPT, Azure OpenAI, Gemini, or local models via Ollama), groups them by screen and
theme, and renders a browsable HTML report. Nothing is uploaded; recordings stay
on your machine.
.PP
The HTML report includes sections (grouped by screen), themes (cross-participant
patterns), a sentiment histogram, friction points, user journeys, clickable
timecodes with a popout video player, favourite quotes with CSV export, inline
editing, and a tag system with auto-suggest.
.SH COMMANDS
.TP
.B run
Full pipeline: ingest files, transcribe, identify speakers, extract quotes,
cluster by screen, group by theme, update participant registry, render HTML and
Markdown reports. Requires an API key. PII redaction is off by default; use
\fB\-\-redact\-pii\fR to enable.
.TP
.B transcribe
Transcription only. No LLM calls, no API key needed. Produces raw transcript
files in \fIbristlenose-output/transcripts-raw/\fR.
.TP
.B analyze
Run LLM analysis on existing transcript \fI.txt\fR files. Skips ingestion and
transcription. Useful after reviewing or editing raw transcripts.
.TP
.B analyse
British English alias for \fBanalyze\fR.
.TP
.B render
Re-render reports from existing intermediate JSON data. No transcription, no LLM
calls, no API key needed. Useful after CSS or JavaScript changes.
.TP
.B doctor
Check runtime dependencies and show tailored fix instructions. Tests FFmpeg,
transcription backend, Whisper model cache, API key validity, network
connectivity, PII redaction dependencies, and disk space. Also runs automatically
on first invocation and as a pre-flight check before pipeline commands.
.TP
.B configure \fIprovider\fR [\fB\-k\fR \fIKEY\fR]
Store API credentials securely in the system credential store. Provider can be
\fBclaude\fR (or \fBanthropic\fR), \fBchatgpt\fR (or \fBopenai\fR, \fBgpt\fR),
\fBazure\fR (or \fBazure-openai\fR), or \fBgemini\fR (or \fBgoogle\fR).
The key is validated with a test API call before storing (Azure validates
via \fBbristlenose doctor\fR instead, since it also needs endpoint and
deployment). Use \fB\-k\fR to provide the key non-interactively.
.TP
.B help \fR[\fItopic\fR]
Show detailed help. Topics: \fBcommands\fR, \fBconfig\fR, \fBworkflows\fR, or
any command name.
.SH OPTIONS
.SS Global options
.TP
.BR \-V ", " \-\-version
Show version and exit.
.TP
.BR \-\-help
Show command help and exit.
.SS Options for \fBrun\fR
.TP
.BR \-o ", " \-\-output " " \fIDIR\fR
Output directory. Default: \fIbristlenose-output/\fR inside the input folder.
.TP
.BR \-p ", " \-\-project " " \fINAME\fR
Project name for the report header. Defaults to the input folder name.
.TP
.BR \-b ", " \-\-whisper\-backend " " \fIBACKEND\fR
Transcription backend: \fBauto\fR (detect hardware), \fBmlx\fR (Apple Silicon GPU),
\fBfaster-whisper\fR (CUDA/CPU). Default: \fBauto\fR.
.TP
.BR \-w ", " \-\-whisper\-model " " \fIMODEL\fR
Whisper model size: \fBtiny\fR, \fBbase\fR, \fBsmall\fR, \fBmedium\fR,
\fBlarge-v3\fR, \fBlarge-v3-turbo\fR. Default: \fBlarge-v3-turbo\fR.
.TP
.BR \-l ", " \-\-llm " " \fIPROVIDER\fR
LLM provider: \fBanthropic\fR (or \fBclaude\fR), \fBopenai\fR (or \fBchatgpt\fR),
\fBazure\fR (or \fBazure-openai\fR), \fBgoogle\fR (or \fBgemini\fR),
\fBlocal\fR (or \fBollama\fR). Default:
\fBanthropic\fR. The \fBgoogle\fR provider uses Google Gemini (budget option).
The \fBlocal\fR provider uses Ollama for free, private, offline
analysis. The \fBazure\fR provider uses Azure OpenAI Service for enterprise
environments.
.TP
.BR \-\-redact\-pii
Redact personally identifying information from transcripts. PII is retained by
default.
.TP
.BR \-\-retain\-pii
Retain PII in transcripts (default behaviour). Mutually exclusive with
\fB\-\-redact\-pii\fR.
.TP
.BR \-\-skip\-transcription
Skip audio transcription (use existing transcripts).
.TP
.BR \-c ", " \-\-config " " \fIPATH\fR
Path to a \fIbristlenose.toml\fR config file.
.TP
.BR \-\-clean
Delete the output directory before running.
.TP
.BR \-v ", " \-\-verbose
Enable verbose logging.
.SS Options for \fBtranscribe\fR
.TP
.BR \-o ", " \-\-output " " \fIDIR\fR
Output directory.
.TP
.BR \-w ", " \-\-whisper\-model " " \fIMODEL\fR
Whisper model size.
.TP
.BR \-v ", " \-\-verbose
Enable verbose logging.
.SS Options for \fBanalyze\fR
.TP
.BR \-o ", " \-\-output " " \fIDIR\fR
Output directory.
.TP
.BR \-p ", " \-\-project " " \fINAME\fR
Project name.
.TP
.BR \-l ", " \-\-llm " " \fIPROVIDER\fR
LLM provider.
.TP
.BR \-v ", " \-\-verbose
Enable verbose logging.
.SS Options for \fBrender\fR
.TP
.BR \-i ", " \-\-input " " \fIDIR\fR
Original input directory for video linking. Auto-detected if not specified.
.TP
.BR \-p ", " \-\-project " " \fINAME\fR
Project name.
.TP
.BR \-\-clean
Accepted for consistency but ignored \(em render is always non-destructive
(overwrites reports only, never touches transcripts, people.yaml, or
intermediate data).
.TP
.BR \-v ", " \-\-verbose
Enable verbose logging.
.SS Options for \fBconfigure\fR
.TP
.BR \-k ", " \-\-key " " \fIKEY\fR
API key to store. If not provided, prompts interactively.
.SH CONFIGURATION
Settings are loaded in order (last wins):
.IP 1. 4
Built-in defaults
.IP 2. 4
\fI.env\fR file (searched upward from the current directory)
.IP 3. 4
Environment variables with prefix \fBBRISTLENOSE_\fR
.IP 4. 4
CLI flags
.PP
API keys have special priority: system credential store \(-> environment variable \(->
\fI.env\fR file. Use \fBbristlenose configure claude\fR, \fBbristlenose configure chatgpt\fR,
\fBbristlenose configure azure\fR, or \fBbristlenose configure gemini\fR to store keys
securely in the system credential store (macOS Keychain, Linux Secret Service).
.SS API keys
.TP
.B BRISTLENOSE_ANTHROPIC_API_KEY
Anthropic API key.
.TP
.B BRISTLENOSE_OPENAI_API_KEY
OpenAI API key.
.TP
.B BRISTLENOSE_GOOGLE_API_KEY
Google Gemini API key.
.SS Azure OpenAI settings
.TP
.B BRISTLENOSE_AZURE_API_KEY
Azure OpenAI API key (from the Azure portal).
.TP
.B BRISTLENOSE_AZURE_ENDPOINT
Azure OpenAI endpoint URL (e.g. \fBhttps://my-resource.openai.azure.com/\fR).
.TP
.B BRISTLENOSE_AZURE_DEPLOYMENT
Azure OpenAI deployment name (defined in the Azure portal, not a model name).
.TP
.B BRISTLENOSE_AZURE_API_VERSION
Azure OpenAI API version. Default: \fB2024-10-21\fR.
.SS LLM settings
.TP
.B BRISTLENOSE_LLM_PROVIDER
\fBanthropic\fR, \fBopenai\fR, \fBazure\fR, \fBgoogle\fR, or \fBlocal\fR. Default: \fBanthropic\fR.
.TP
.B BRISTLENOSE_LLM_MODEL
Model name. Default: \fBclaude-sonnet-4-20250514\fR.
.TP
.B BRISTLENOSE_LLM_MAX_TOKENS
Maximum response tokens. Default: \fB8192\fR.
.TP
.B BRISTLENOSE_LLM_TEMPERATURE
Sampling temperature. Default: \fB0.1\fR.
.TP
.B BRISTLENOSE_LLM_CONCURRENCY
Parallel LLM calls. Default: \fB3\fR.
.SS Local LLM settings (Ollama)
When no API key is configured and you run bristlenose, it offers an interactive
choice: Local AI (free via Ollama), Claude, or ChatGPT. If you choose Local and
Ollama isn't installed, bristlenose offers to install it automatically (Homebrew
on macOS, snap on Linux, curl script fallback). If Ollama is installed but not
running, bristlenose will start it for you.
.TP
.B BRISTLENOSE_LOCAL_URL
Ollama API endpoint. Default: \fBhttp://localhost:11434/v1\fR.
.TP
.B BRISTLENOSE_LOCAL_MODEL
Local model name. Default: \fBllama3.2:3b\fR. Other options: \fBllama3.1:8b\fR,
\fBmistral:7b\fR, \fBqwen2.5:7b\fR.
.SS Transcription settings
.TP
.B BRISTLENOSE_WHISPER_BACKEND
\fBauto\fR, \fBmlx\fR, or \fBfaster-whisper\fR. Default: \fBauto\fR.
.TP
.B BRISTLENOSE_WHISPER_MODEL
Model size. Default: \fBlarge-v3-turbo\fR.
.TP
.B BRISTLENOSE_WHISPER_LANGUAGE
Language code. Default: \fBen\fR.
.TP
.B BRISTLENOSE_WHISPER_DEVICE
Device for faster-whisper: \fBcpu\fR, \fBcuda\fR, or \fBauto\fR. Default: \fBauto\fR.
.TP
.B BRISTLENOSE_WHISPER_COMPUTE_TYPE
Compute type for faster-whisper: \fBint8\fR, \fBfloat16\fR, or \fBfloat32\fR. Default: \fBint8\fR.
.SS PII settings
.TP
.B BRISTLENOSE_PII_ENABLED
Enable PII redaction. Default: \fBfalse\fR.
.TP
.B BRISTLENOSE_PII_LLM_PASS
Extra LLM-based PII pass. Default: \fBfalse\fR.
.TP
.B BRISTLENOSE_PII_CUSTOM_NAMES
Comma-separated list of additional names to redact.
.SS Pipeline settings
.TP
.B BRISTLENOSE_MIN_QUOTE_WORDS
Minimum words per extracted quote. Default: \fB5\fR.
.TP
.B BRISTLENOSE_MERGE_SPEAKER_GAP_SECONDS
Gap threshold for merging consecutive speaker segments. Default: \fB2.0\fR.
.SH INPUT FILES
.B bristlenose
accepts any mix of audio, video, subtitle, and transcript files:
.PP
.RS
.nf
Audio:       .wav .mp3 .m4a .flac .ogg .wma .aac
Video:       .mp4 .mov .avi .mkv .webm
Subtitles:   .srt .vtt
Transcripts: .docx (Microsoft Teams exports), .txt
.fi
.RE
.PP
Files sharing a name stem (e.g. \fIp1.mp4\fR and \fIp1.srt\fR) are treated as
a single session. When subtitles or transcripts exist, transcription is skipped
for that session.
.PP
Platform naming conventions are auto-detected for session grouping: Microsoft
Teams recording/transcript suffixes, Zoom cloud prefixes and meeting IDs, Zoom
local recording folders, and Google Meet parenthetical timestamps are all
stripped before stem comparison.
.SH OUTPUT
Output goes inside the input folder by default (avoids collisions when
processing multiple projects):
.PP
.RS
.nf
interviews/                          input folder
  bristlenose-output/                output folder (inside input)
    bristlenose-interviews-report.html    browsable report
    bristlenose-interviews-report.md      Markdown version
    people.yaml                           participant registry
    assets/
      bristlenose-theme.css               stylesheet
      bristlenose-logo.png                logo (light)
      bristlenose-logo-dark.png           logo (dark)
      bristlenose-player.html             popout video player
    sessions/
      transcript_s1.html                  per-session transcript pages
      transcript_s2.html
    transcripts-raw/                      one .txt and .md per session
    transcripts-cooked/                   PII-redacted (only with --redact-pii)
    .bristlenose/
      intermediate/                       JSON snapshots for re-rendering
      temp/                               FFmpeg scratch files
.fi
.RE
.SH EXAMPLES
Full pipeline run (output created inside input folder):
.PP
.RS
.nf
bristlenose run ./interviews/
# Creates ./interviews/bristlenose-output/
.fi
.RE
.PP
Full pipeline with custom output location:
.PP
.RS
.nf
bristlenose run ./interviews/ -o ./results/ -p "Q1 Usability Study"
.fi
.RE
.PP
Transcribe only (no API key needed):
.PP
.RS
.nf
bristlenose transcribe ./interviews/
.fi
.RE
.PP
Use Azure OpenAI (enterprise):
.PP
.RS
.nf
export BRISTLENOSE_AZURE_ENDPOINT=https://my-resource.openai.azure.com/
export BRISTLENOSE_AZURE_DEPLOYMENT=gpt-4o-research
bristlenose configure azure
bristlenose run ./interviews/ --llm azure
.fi
.RE
.PP
Use Gemini (budget option):
.PP
.RS
.nf
bristlenose configure gemini
bristlenose run ./interviews/ --llm gemini
.fi
.RE
.PP
Use local AI via Ollama (free, no API key):
.PP
.RS
.nf
bristlenose run ./interviews/ --llm local
# Requires Ollama running: ollama serve
.fi
.RE
.PP
Analyse existing transcripts:
.PP
.RS
.nf
bristlenose analyze ./interviews/bristlenose-output/transcripts-raw/
.fi
.RE
.PP
Re-render after CSS changes (no API key needed):
.PP
.RS
.nf
cd ./interviews/ && bristlenose render
# Or specify explicitly:
bristlenose render ./interviews/bristlenose-output/
.fi
.RE
.PP
Check dependencies and setup:
.PP
.RS
.nf
bristlenose doctor
.fi
.RE
.PP
Show configuration reference:
.PP
.RS
.nf
bristlenose help config
.fi
.RE
.SH HARDWARE
Transcription hardware is auto-detected:
.IP \(bu 4
Apple Silicon: MLX on Metal GPU
.IP \(bu 4
NVIDIA: faster-whisper with CUDA
.IP \(bu 4
Other: CPU fallback
.SH FILES
.TP
.I .env
Environment variables file, searched upward from the current directory.
.TP
.I .env.example
Template with all available configuration variables.
.TP
.I bristlenose.toml
Optional TOML configuration file.
.SH EXIT STATUS
.TP
.B 0
Success.
.TP
.B 1
Error (missing input, configuration problem, pipeline failure).
.SH BUGS
Report bugs at \fIhttps://github.com/cassiocassio/bristlenose/issues\fR.
.SH AUTHOR
Cassio \fIhttps://github.com/cassiocassio\fR
.SH LICENSE
AGPL-3.0. See the LICENSE file in the source distribution.
.SH SEE ALSO
.BR ffmpeg (1),
.BR whisper (1)
.PP
Full documentation: \fIhttps://github.com/cassiocassio/bristlenose\fR
